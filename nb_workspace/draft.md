Clusterize the customers, how many unique customers per unique articles.

Timeseries [Trend, seasonality]
- Summer vs winter, best sellling articles, total number of sales.

Percentage of customers interested in special sizes [xxs, xxl].
- Users tend to choise stores with varaity in sizes.
- Adding extra features in sizes, increase the sales or not.

We have already huge number of customers in our data who don't use promos to buy, So we should go dig deep more in these category of customers to investigate more about what they are interested in, and set a special promo for them.

Also we need to invistigate the small percent of customers who bought with promo.

Recommendation:

A/B testing, also known as split testing, is a method used to compare two or more variations of a webpage or app against each other to determine which one performs better. It is commonly used in marketing and product development to make data-driven decisions and optimize performance. Here's a step-by-step guide on how to perform A/B testing analysis:

    Define the Objective: Clearly define the goal of the A/B test. It could be to improve click-through rates, conversion rates, user engagement, or any other relevant metric.

    Select the Variations: Decide on the variations (A and B) that you want to test. These can be different designs, layouts, content, or features.

    Random Sampling: Randomly divide your audience into two groups: Group A (control group) and Group B (test group). Ensure that the groups are large enough to provide statistically significant results.

    Run the Test: Present Group A with the original (current) version, and Group B with the modified version (the one you want to test). Allow sufficient time for both groups to interact with the variations.

    Collect Data: Measure and collect relevant data for each group. This could be the number of clicks, conversions, or any other metric that aligns with your objective.

    Statistical Analysis: Use statistical methods (e.g., t-tests, chi-squared tests) to analyze the data and determine if there is a significant difference between the two variations.

    Interpret the Results: Analyze the results and determine which variation performed better based on the chosen metric. Consider statistical significance and practical significance.

    Draw Conclusions: Based on the results, draw conclusions about which variation is more effective and whether it is worth implementing the changes permanently.

    Implement the Winning Variation: If one variation outperforms the other and the results are statistically significant, implement the winning variation.

    Continue Monitoring: Continue to monitor the performance of the implemented changes to ensure they continue to be effective.

Remember that A/B testing should be done carefully and rigorously to ensure accurate results. Random sampling, statistical significance, and sufficient sample size are crucial for reliable conclusions.

Additionally, it's essential to ensure that the variations being tested are truly independent and do not interfere with each other's results. Avoid testing multiple variations simultaneously, as it can introduce biases and make it difficult to isolate the impact of individual changes.

A/B testing is an iterative process, and it is common to run multiple tests to fine-tune designs and strategies continually. By using A/B testing analysis, businesses can optimize their marketing campaigns, website/app designs, and user experiences to drive better results and achieve their objectives.

